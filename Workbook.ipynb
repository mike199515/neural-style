{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = tf.constant(target_g, dtype=tf.float32)\n",
    "# bases = tf.constant(bases_g, dtype=tf.float32)\n",
    "def make_symmetric(feat_dim):\n",
    "    a = np.random.normal(size=(feat_dim, feat_dim))\n",
    "    return np.matmul(a.T, a)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "feat_dim = 500\n",
    "base_size = 5\n",
    "target_g = make_symmetric(feat_dim)\n",
    "bases_g = np.zeros((base_size, feat_dim, feat_dim))\n",
    "for i in range(base_size):\n",
    "    bases_g[i,:,:] = make_symmetric(feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 500, 500), (500, 500))"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bases_g.shape, target_g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tf.placeholder(tf.float32, [feat_dim, feat_dim])\n",
    "bases = tf.placeholder(tf.float32, [None, feat_dim, feat_dim])\n",
    "alphas = tf.get_variable(\"a\", shape=[base_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_bases = bases * tf.expand_dims(tf.expand_dims(alphas, -1), -1)\n",
    "approx_base = tf.reduce_sum(scaled_bases, axis=0)\n",
    "diff = target - approx_base\n",
    "loss = tf.norm(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sub:0' shape=(500, 500) dtype=float32>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  16895.303\n",
      "Alpha:  [-0.34949914  0.09934513  0.19076324  0.05459074  0.3858446 ]\n",
      "Loss:  14567.475\n",
      "Alpha:  [-0.26709914  0.1435545   0.22741498  0.10224678  0.40561932]\n",
      "Loss:  13539.104\n",
      "Alpha:  [-0.20726632  0.16573392  0.24210915  0.12782012  0.40371838]\n",
      "Loss:  13127.868\n",
      "Alpha:  [-0.16265929  0.17504328  0.24437247  0.14030288  0.39044896]\n",
      "Loss:  12910.704\n",
      "Alpha:  [-0.12693977  0.17826276  0.24107958  0.14643432  0.37286595]\n",
      "Loss:  12756.113\n",
      "Alpha:  [-0.09662326  0.17885596  0.2356951   0.14968047  0.35441753]\n",
      "Loss:  12632.423\n",
      "Alpha:  [-0.07000123  0.1783962   0.22976944  0.15162797  0.33659145]\n",
      "Loss:  12530.782\n",
      "Alpha:  [-0.04622719  0.17756432  0.22395378  0.152974    0.31996462]\n",
      "Loss:  12447.072\n",
      "Alpha:  [-0.0248424   0.17664413  0.21849988  0.15401947  0.30471125]\n",
      "Loss:  12378.323\n",
      "Alpha:  [-0.005558    0.17574824  0.21348764  0.15489416  0.29083344]\n",
      "Loss:  12322.045\n",
      "Alpha:  [0.01183978 0.17491731 0.20892526 0.15565543 0.2782632 ]\n",
      "Loss:  12276.118\n",
      "Alpha:  [0.02752883 0.1741625  0.20479248 0.15633045 0.26690698]\n",
      "Loss:  12238.732\n",
      "Alpha:  [0.04166706 0.17348339 0.20105873 0.15693386 0.2566645 ]\n",
      "Loss:  12208.361\n",
      "Alpha:  [0.05439839 0.17287518 0.1976908  0.15747498 0.24743724]\n",
      "Loss:  12183.73\n",
      "Alpha:  [0.06585519 0.17233181 0.19465601 0.15796074 0.23913163]\n",
      "Loss:  12163.787\n",
      "Alpha:  [0.0761591  0.17184703 0.19192344 0.1583969  0.23166053]\n",
      "Loss:  12147.657\n",
      "Alpha:  [0.08542164 0.1714149  0.18946435 0.15878844 0.22494356]\n",
      "Loss:  12134.625\n",
      "Alpha:  [0.09374475 0.17102993 0.1872523  0.15913984 0.21890712]\n",
      "Loss:  12124.096\n",
      "Alpha:  [0.10122128 0.17068714 0.18526316 0.15945514 0.21348403]\n",
      "Loss:  12115.605\n",
      "Alpha:  [0.10793556 0.17038202 0.18347494 0.15973796 0.20861329]\n",
      "Loss:  12108.759\n",
      "Alpha:  [0.11396398 0.17011051 0.18186772 0.1599916  0.20423959]\n",
      "Loss:  12103.233\n",
      "Alpha:  [0.11937571 0.16986898 0.18042342 0.16021904 0.20031287]\n",
      "Loss:  12098.783\n",
      "Alpha:  [0.12423311 0.16965415 0.17912571 0.16042297 0.19678795]\n",
      "Loss:  12095.198\n",
      "Alpha:  [0.1285925  0.16946311 0.17795986 0.16060577 0.19362411]\n",
      "Loss:  12092.309\n",
      "Alpha:  [0.13250458 0.16929325 0.17691255 0.16076963 0.19078459]\n",
      "Loss:  12089.98\n",
      "Alpha:  [0.13601497 0.16914226 0.17597179 0.16091649 0.18823633]\n",
      "Loss:  12088.113\n",
      "Alpha:  [0.13916473 0.16900806 0.17512682 0.16104813 0.1859496 ]\n",
      "Loss:  12086.604\n",
      "Alpha:  [0.1419908  0.16888879 0.1743679  0.1611661  0.18389764]\n",
      "Loss:  12085.388\n",
      "Alpha:  [0.14452632 0.1687828  0.17368633 0.16127183 0.18205646]\n",
      "Loss:  12084.412\n",
      "Alpha:  [0.1468011  0.16868863 0.1730742  0.16136657 0.18040442]\n",
      "Loss:  12083.621\n",
      "Alpha:  [0.1488419  0.16860496 0.17252448 0.16145146 0.17892215]\n",
      "Loss:  12082.991\n",
      "Alpha:  [0.15067275 0.16853063 0.1720308  0.16152753 0.17759222]\n",
      "Loss:  12082.48\n",
      "Alpha:  [0.15231523 0.16846462 0.17158748 0.1615957  0.17639899]\n",
      "Loss:  12082.069\n",
      "Alpha:  [0.1537887  0.168406   0.17118937 0.16165678 0.17532843]\n",
      "Loss:  12081.74\n",
      "Alpha:  [0.15511054 0.16835393 0.17083187 0.16171151 0.17436793]\n",
      "Loss:  12081.474\n",
      "Alpha:  [0.15629633 0.16830769 0.17051084 0.16176055 0.17350619]\n",
      "Loss:  12081.258\n",
      "Alpha:  [0.15736008 0.16826665 0.17022257 0.1618045  0.17273305]\n",
      "Loss:  12081.087\n",
      "Alpha:  [0.15831433 0.1682302  0.1699637  0.16184388 0.17203942]\n",
      "Loss:  12080.95\n",
      "Alpha:  [0.15917037 0.16819786 0.16973126 0.16187917 0.17141712]\n",
      "Loss:  12080.84\n",
      "Alpha:  [0.15993829 0.16816914 0.16952252 0.16191079 0.17085882]\n",
      "Loss:  12080.748\n",
      "Alpha:  [0.16062716 0.16814366 0.16933508 0.16193911 0.17035793]\n",
      "Loss:  12080.676\n",
      "Alpha:  [0.16124512 0.16812104 0.16916677 0.16196449 0.16990855]\n",
      "Loss:  12080.617\n",
      "Alpha:  [0.16179948 0.16810097 0.16901565 0.16198723 0.16950539]\n",
      "Loss:  12080.572\n",
      "Alpha:  [0.16229676 0.16808316 0.16887994 0.16200761 0.16914369]\n",
      "Loss:  12080.531\n",
      "Alpha:  [0.16274285 0.16806737 0.16875808 0.16202587 0.16881919]\n",
      "Loss:  12080.501\n",
      "Alpha:  [0.16314302 0.16805336 0.16864866 0.16204223 0.16852807]\n",
      "Loss:  12080.479\n",
      "Alpha:  [0.163502   0.16804093 0.1685504  0.1620569  0.16826688]\n",
      "Loss:  12080.458\n",
      "Alpha:  [0.163824   0.1680299  0.16846217 0.16207002 0.16803256]\n",
      "Loss:  12080.442\n",
      "Alpha:  [0.16411288 0.16802013 0.16838296 0.1620818  0.16782233]\n",
      "Loss:  12080.43\n",
      "Alpha:  [0.16437201 0.16801147 0.16831182 0.16209234 0.16763373]\n",
      "Loss:  12080.419\n",
      "Alpha:  [0.16460447 0.1680038  0.16824795 0.16210179 0.16746452]\n",
      "Loss:  12080.412\n",
      "Alpha:  [0.164813   0.16799699 0.1681906  0.16211025 0.16731271]\n",
      "Loss:  12080.404\n",
      "Alpha:  [0.16500005 0.16799095 0.1681391  0.16211784 0.16717651]\n",
      "Loss:  12080.399\n",
      "Alpha:  [0.16516785 0.1679856  0.16809286 0.16212463 0.16705434]\n",
      "Loss:  12080.395\n",
      "Alpha:  [0.16531838 0.16798086 0.16805135 0.16213073 0.16694473]\n",
      "Loss:  12080.393\n",
      "Alpha:  [0.16545342 0.16797666 0.16801406 0.16213618 0.16684638]\n",
      "Loss:  12080.387\n",
      "Alpha:  [0.16557455 0.16797295 0.1679806  0.16214107 0.16675815]\n",
      "Loss:  12080.386\n",
      "Alpha:  [0.16568321 0.16796966 0.16795054 0.16214545 0.166679  ]\n",
      "Loss:  12080.384\n",
      "Alpha:  [0.1657807  0.16796674 0.16792354 0.16214937 0.16660798]\n",
      "Loss:  12080.383\n",
      "Alpha:  [0.16586813 0.16796416 0.16789931 0.16215289 0.16654427]\n",
      "Loss:  12080.382\n",
      "Alpha:  [0.16594657 0.16796188 0.16787755 0.16215603 0.16648711]\n",
      "Loss:  12080.38\n",
      "Alpha:  [0.16601694 0.16795985 0.16785802 0.16215885 0.16643584]\n",
      "Loss:  12080.381\n",
      "Alpha:  [0.16608006 0.16795807 0.16784047 0.16216138 0.16638984]\n",
      "Loss:  12080.381\n",
      "Alpha:  [0.16613668 0.16795649 0.16782472 0.16216365 0.16634856]\n",
      "Loss:  12080.379\n",
      "Alpha:  [0.16618748 0.16795509 0.16781057 0.16216567 0.16631153]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16623305 0.16795385 0.16779786 0.16216749 0.16627832]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16627392 0.16795275 0.16778646 0.16216911 0.16624852]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.1663106  0.16795178 0.16777623 0.16217057 0.16622178]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16634348 0.16795093 0.16776703 0.16217189 0.16619779]\n",
      "Loss:  12080.379\n",
      "Alpha:  [0.16637298 0.16795017 0.16775878 0.16217306 0.16617627]\n",
      "Loss:  12080.38\n",
      "Alpha:  [0.16639945 0.1679495  0.16775136 0.1621741  0.16615696]\n",
      "Loss:  12080.38\n",
      "Alpha:  [0.1664232  0.1679489  0.1677447  0.16217504 0.16613963]\n",
      "Loss:  12080.379\n",
      "Alpha:  [0.16644451 0.16794838 0.16773872 0.1621759  0.16612409]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16646361 0.16794792 0.16773336 0.16217665 0.16611014]\n",
      "Loss:  12080.377\n",
      "Alpha:  [0.16648075 0.16794752 0.16772854 0.16217734 0.16609764]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16649613 0.16794716 0.16772422 0.16217795 0.16608642]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16650991 0.16794685 0.16772033 0.16217849 0.16607635]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16652228 0.16794656 0.16771685 0.16217898 0.16606732]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16653338 0.16794631 0.16771372 0.16217941 0.16605921]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16654333 0.1679461  0.1677109  0.1621798  0.16605194]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16655228 0.1679459  0.16770838 0.16218016 0.16604541]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16656029 0.16794574 0.16770612 0.16218047 0.16603956]\n",
      "Loss:  12080.379\n",
      "Alpha:  [0.16656747 0.1679456  0.16770408 0.16218075 0.16603431]\n",
      "Loss:  12080.38\n",
      "Alpha:  [0.16657393 0.16794546 0.16770224 0.162181   0.1660296 ]\n",
      "Loss:  12080.38\n",
      "Alpha:  [0.16657971 0.16794534 0.1677006  0.16218123 0.16602537]\n",
      "Loss:  12080.381\n",
      "Alpha:  [0.1665849  0.16794524 0.16769913 0.16218144 0.16602159]\n",
      "Loss:  12080.38\n",
      "Alpha:  [0.16658956 0.16794515 0.1676978  0.16218162 0.16601819]\n",
      "Loss:  12080.379\n",
      "Alpha:  [0.16659373 0.16794507 0.16769661 0.16218178 0.16601513]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16659747 0.16794501 0.16769555 0.16218193 0.16601239]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16660084 0.16794495 0.1676946  0.16218206 0.16600993]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16660385 0.1679449  0.16769373 0.16218218 0.16600773]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16660656 0.16794485 0.16769296 0.16218229 0.16600575]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16660899 0.1679448  0.16769227 0.16218238 0.16600397]\n",
      "Loss:  12080.379\n",
      "Alpha:  [0.16661116 0.16794477 0.16769165 0.16218245 0.16600238]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16661312 0.16794474 0.1676911  0.16218252 0.16600095]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16661488 0.16794471 0.16769059 0.16218258 0.16599967]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16661644 0.1679447  0.16769014 0.16218264 0.16599852]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16661786 0.16794468 0.16768974 0.1621827  0.16599749]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16661912 0.16794467 0.16768938 0.16218275 0.16599657]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16662025 0.16794465 0.16768906 0.1621828  0.16599573]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16662127 0.16794464 0.16768876 0.16218284 0.16599499]\n",
      "Loss:  12080.377\n",
      "Alpha:  [0.16662218 0.16794463 0.16768849 0.16218287 0.16599432]\n",
      "Loss:  12080.377\n",
      "Alpha:  [0.166623   0.16794461 0.16768825 0.1621829  0.16599372]\n",
      "Loss:  12080.377\n",
      "Alpha:  [0.16662373 0.1679446  0.16768804 0.16218293 0.16599318]\n",
      "Loss:  12080.377\n",
      "Alpha:  [0.16662438 0.1679446  0.16768785 0.16218296 0.1659927 ]\n",
      "Loss:  12080.378\n",
      "Alpha:  [0.16662498 0.16794458 0.16768768 0.16218299 0.16599227]\n",
      "Loss:  12080.377\n",
      "Alpha:  [0.1666255  0.16794457 0.16768754 0.162183   0.16599189]\n",
      "Loss:  12080.377\n",
      "Alpha:  [0.16662598 0.16794457 0.1676874  0.16218302 0.16599154]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662641 0.16794457 0.16768728 0.16218303 0.16599123]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.1666268  0.16794455 0.16768716 0.16218305 0.16599095]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662714 0.16794455 0.16768706 0.16218306 0.1659907 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  12080.376\n",
      "Alpha:  [0.16662745 0.16794455 0.16768697 0.16218308 0.16599047]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662772 0.16794455 0.16768688 0.16218309 0.16599026]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662797 0.16794455 0.1676868  0.1621831  0.16599008]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.1666282  0.16794455 0.16768675 0.1621831  0.16598992]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662839 0.16794455 0.16768669 0.1621831  0.16598977]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662857 0.16794455 0.16768663 0.16218312 0.16598964]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662873 0.16794455 0.16768658 0.16218314 0.16598952]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662888 0.16794455 0.16768654 0.16218314 0.16598941]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662902 0.16794455 0.16768649 0.16218314 0.16598932]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662914 0.16794455 0.16768646 0.16218314 0.16598924]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662924 0.16794455 0.16768643 0.16218314 0.16598916]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662933 0.16794455 0.1676864  0.16218314 0.16598909]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662942 0.16794455 0.16768637 0.16218314 0.16598903]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.1666295  0.16794455 0.16768636 0.16218314 0.16598897]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662955 0.16794455 0.16768634 0.16218314 0.16598892]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662961 0.16794455 0.16768633 0.16218314 0.16598888]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662967 0.16794455 0.16768631 0.16218314 0.16598883]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662972 0.16794455 0.1676863  0.16218314 0.1659888 ]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662976 0.16794455 0.16768628 0.16218314 0.16598877]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662979 0.16794455 0.16768627 0.16218314 0.16598874]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662982 0.16794455 0.16768625 0.16218315 0.16598873]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662985 0.16794455 0.16768624 0.16218315 0.16598871]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662988 0.16794455 0.16768624 0.16218315 0.1659887 ]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662991 0.16794455 0.16768622 0.16218315 0.16598868]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662993 0.16794455 0.16768622 0.16218315 0.16598867]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662994 0.16794455 0.16768622 0.16218315 0.16598865]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662996 0.16794455 0.16768622 0.16218315 0.16598864]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662997 0.16794455 0.16768622 0.16218315 0.16598862]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16662998 0.16794455 0.16768622 0.16218315 0.16598861]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663    0.16794455 0.16768622 0.16218315 0.1659886 ]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663001 0.16794455 0.16768622 0.16218315 0.16598858]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663003 0.16794455 0.16768622 0.16218315 0.16598856]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663004 0.16794455 0.16768622 0.16218315 0.16598856]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663004 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n",
      "Loss:  12080.376\n",
      "Alpha:  [0.16663006 0.16794455 0.16768621 0.16218315 0.16598855]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Feeding data!\n",
    "    for i in range(400):\n",
    "        _, loss_val, alpha_vals, diff_vals = sess.run([optimizer, loss, alphas, diff], feed_dict={target: target_g, bases: bases_g}) \n",
    "        print(\"Loss: \", loss_val)\n",
    "        print(\"Alpha: \", alpha_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.zeros_like(target_g)\n",
    "for i in range(base_size):\n",
    "    combined += alpha_vals[i] * bases_g[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isclose(combined.T , combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.97939023,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.4781759 , 18.27943098,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.57829228,  0.10517567, 17.79801752, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.72975176, -0.05929904,  0.18329624, ..., 13.09117947,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.40674864,  0.60597524,  1.51915034, ..., -0.36643433,\n",
       "        12.20004142,  0.        ],\n",
       "       [ 0.06141995,  0.94357914,  0.7633443 , ...,  0.46030242,\n",
       "        -0.46842785, 12.71503253]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cholesky(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2015-2018 Anish Athalye. Released under GPLv3.\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "\n",
    "from stylize import stylize\n",
    "\n",
    "import math\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --content CONTENT --styles STYLE [STYLE ...]\n",
      "                             --output OUTPUT [--iterations ITERATIONS]\n",
      "                             [--print-iterations PRINT_ITERATIONS]\n",
      "                             [--checkpoint-output OUTPUT]\n",
      "                             [--checkpoint-iterations CHECKPOINT_ITERATIONS]\n",
      "                             [--width WIDTH]\n",
      "                             [--style-scales STYLE_SCALE [STYLE_SCALE ...]]\n",
      "                             [--network VGG_PATH]\n",
      "                             [--content-weight-blend CONTENT_WEIGHT_BLEND]\n",
      "                             [--content-weight CONTENT_WEIGHT]\n",
      "                             [--style-weight STYLE_WEIGHT]\n",
      "                             [--style-layer-weight-exp STYLE_LAYER_WEIGHT_EXP]\n",
      "                             [--style-blend-weights STYLE_BLEND_WEIGHT [STYLE_BLEND_WEIGHT ...]]\n",
      "                             [--tv-weight TV_WEIGHT]\n",
      "                             [--color-weight COLOR_WEIGHT]\n",
      "                             [--affine-weight AFFINE_WEIGHT]\n",
      "                             [--edge-weight EDGE_WEIGHT]\n",
      "                             [--learning-rate LEARNING_RATE] [--beta1 BETA1]\n",
      "                             [--beta2 BETA2] [--eps EPSILON]\n",
      "                             [--initial INITIAL]\n",
      "                             [--initial-noiseblend INITIAL_NOISEBLEND]\n",
      "                             [--preserve-colors] [--pooling POOLING]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --content, --styles, --output\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mick/anaconda/envs/ml13/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# default arguments\n",
    "CONTENT_WEIGHT = 5e0\n",
    "CONTENT_WEIGHT_BLEND = 1\n",
    "STYLE_WEIGHT = 1e2\n",
    "TV_WEIGHT = 1e2\n",
    "COLOR_WEIGHT = 0\n",
    "AFFINE_WEIGHT = 0\n",
    "EDGE_WEIGHT = 1e8\n",
    "STYLE_LAYER_WEIGHT_EXP = 1\n",
    "LEARNING_RATE = 1e1\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1e-08\n",
    "STYLE_SCALE = 1.0\n",
    "ITERATIONS = 1000\n",
    "VGG_PATH = 'imagenet-vgg-verydeep-19.mat'\n",
    "POOLING = 'max'\n",
    "\n",
    "def build_parser():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--content',\n",
    "            dest='content', help='content image',\n",
    "            metavar='CONTENT', required=True)\n",
    "    parser.add_argument('--styles',\n",
    "            dest='styles',\n",
    "            nargs='+', help='one or more style images',\n",
    "            metavar='STYLE', required=True)\n",
    "    parser.add_argument('--output',\n",
    "            dest='output', help='output path',\n",
    "            metavar='OUTPUT', required=True)\n",
    "    parser.add_argument('--iterations', type=int,\n",
    "            dest='iterations', help='iterations (default %(default)s)',\n",
    "            metavar='ITERATIONS', default=ITERATIONS)\n",
    "    parser.add_argument('--print-iterations', type=int,\n",
    "            dest='print_iterations', help='statistics printing frequency',\n",
    "            metavar='PRINT_ITERATIONS')\n",
    "    parser.add_argument('--checkpoint-output',\n",
    "            dest='checkpoint_output', help='checkpoint output format, e.g. output%%s.jpg',\n",
    "            metavar='OUTPUT')\n",
    "    parser.add_argument('--checkpoint-iterations', type=int,\n",
    "            dest='checkpoint_iterations', help='checkpoint frequency',\n",
    "            metavar='CHECKPOINT_ITERATIONS')\n",
    "    parser.add_argument('--width', type=int,\n",
    "            dest='width', help='output width',\n",
    "            metavar='WIDTH')\n",
    "    parser.add_argument('--style-scales', type=float,\n",
    "            dest='style_scales',\n",
    "            nargs='+', help='one or more style scales',\n",
    "            metavar='STYLE_SCALE')\n",
    "    parser.add_argument('--network',\n",
    "            dest='network', help='path to network parameters (default %(default)s)',\n",
    "            metavar='VGG_PATH', default=VGG_PATH)\n",
    "    parser.add_argument('--content-weight-blend', type=float,\n",
    "            dest='content_weight_blend', help='content weight blend, conv4_2 * blend + conv5_2 * (1-blend) (default %(default)s)',\n",
    "            metavar='CONTENT_WEIGHT_BLEND', default=CONTENT_WEIGHT_BLEND)\n",
    "    parser.add_argument('--content-weight', type=float,\n",
    "            dest='content_weight', help='content weight (default %(default)s)',\n",
    "            metavar='CONTENT_WEIGHT', default=CONTENT_WEIGHT)\n",
    "    parser.add_argument('--style-weight', type=float,\n",
    "            dest='style_weight', help='style weight (default %(default)s)',\n",
    "            metavar='STYLE_WEIGHT', default=STYLE_WEIGHT)\n",
    "    parser.add_argument('--style-layer-weight-exp', type=float,\n",
    "            dest='style_layer_weight_exp', help='style layer weight exponentional increase - weight(layer<n+1>) = weight_exp*weight(layer<n>) (default %(default)s)',\n",
    "            metavar='STYLE_LAYER_WEIGHT_EXP', default=STYLE_LAYER_WEIGHT_EXP)\n",
    "    parser.add_argument('--style-blend-weights', type=float,\n",
    "            dest='style_blend_weights', help='style blending weights',\n",
    "            nargs='+', metavar='STYLE_BLEND_WEIGHT')\n",
    "    parser.add_argument('--tv-weight', type=float,\n",
    "            dest='tv_weight', help='total variation regularization weight (default %(default)s)',\n",
    "            metavar='TV_WEIGHT', default=TV_WEIGHT)\n",
    "    parser.add_argument('--color-weight', type=float,\n",
    "            dest='color_weight', help='color constraint weight (default %(default)s)',\n",
    "            metavar='COLOR_WEIGHT', default=COLOR_WEIGHT)\n",
    "    parser.add_argument('--affine-weight', type=float,\n",
    "            dest='affine_weight', help='affine constraint weight (default %(default)s)',\n",
    "            metavar='AFFINE_WEIGHT', default=AFFINE_WEIGHT)\n",
    "    parser.add_argument('--edge-weight', type=float,\n",
    "            dest='edge_weight', help='edge constraint weight (default %(default)s)',\n",
    "            metavar='EDGE_WEIGHT', default=EDGE_WEIGHT)\n",
    "    parser.add_argument('--learning-rate', type=float,\n",
    "            dest='learning_rate', help='learning rate (default %(default)s)',\n",
    "            metavar='LEARNING_RATE', default=LEARNING_RATE)\n",
    "    parser.add_argument('--beta1', type=float,\n",
    "            dest='beta1', help='Adam: beta1 parameter (default %(default)s)',\n",
    "            metavar='BETA1', default=BETA1)\n",
    "    parser.add_argument('--beta2', type=float,\n",
    "            dest='beta2', help='Adam: beta2 parameter (default %(default)s)',\n",
    "            metavar='BETA2', default=BETA2)\n",
    "    parser.add_argument('--eps', type=float,\n",
    "            dest='epsilon', help='Adam: epsilon parameter (default %(default)s)',\n",
    "            metavar='EPSILON', default=EPSILON)\n",
    "    parser.add_argument('--initial',\n",
    "            dest='initial', help='initial image',\n",
    "            metavar='INITIAL')\n",
    "    parser.add_argument('--initial-noiseblend', type=float,\n",
    "            dest='initial_noiseblend', help='ratio of blending initial image with normalized noise (if no initial image specified, content image is used) (default %(default)s)',\n",
    "            metavar='INITIAL_NOISEBLEND')\n",
    "    parser.add_argument('--preserve-colors', action='store_true',\n",
    "            dest='preserve_colors', help='style-only transfer (preserving colors) - if color transfer is not needed')\n",
    "    parser.add_argument('--pooling',\n",
    "            dest='pooling', help='pooling layer configuration: max or avg (default %(default)s)',\n",
    "            metavar='POOLING', default=POOLING)\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = build_parser()\n",
    "    options = parser.parse_args()\n",
    "\n",
    "    if not os.path.isfile(options.network):\n",
    "        parser.error(\"Network %s does not exist. (Did you forget to download it?)\" % options.network)\n",
    "\n",
    "    content_image = imread(options.content)\n",
    "    style_images = [imread(style) for style in options.styles]\n",
    "\n",
    "    width = options.width\n",
    "    if width is not None:\n",
    "        new_shape = (int(math.floor(float(content_image.shape[0]) /\n",
    "                content_image.shape[1] * width)), width)\n",
    "        content_image = scipy.misc.imresize(content_image, new_shape)\n",
    "    target_shape = content_image.shape\n",
    "    for i in range(len(style_images)):\n",
    "        style_scale = STYLE_SCALE\n",
    "        if options.style_scales is not None:\n",
    "            style_scale = options.style_scales[i]\n",
    "        style_images[i] = scipy.misc.imresize(style_images[i], style_scale *\n",
    "                target_shape[1] / style_images[i].shape[1])\n",
    "\n",
    "    style_blend_weights = options.style_blend_weights\n",
    "    if style_blend_weights is None:\n",
    "        # default is equal weights\n",
    "        style_blend_weights = [1.0/len(style_images) for _ in style_images]\n",
    "    else:\n",
    "        total_blend_weight = sum(style_blend_weights)\n",
    "        style_blend_weights = [weight/total_blend_weight\n",
    "                               for weight in style_blend_weights]\n",
    "\n",
    "    initial = options.initial\n",
    "    if initial is not None:\n",
    "        initial = scipy.misc.imresize(imread(initial), content_image.shape[:2])\n",
    "        # Initial guess is specified, but not noiseblend - no noise should be blended\n",
    "        if options.initial_noiseblend is None:\n",
    "            options.initial_noiseblend = 0.0\n",
    "    else:\n",
    "        # Neither inital, nor noiseblend is provided, falling back to random generated initial guess\n",
    "        if options.initial_noiseblend is None:\n",
    "            options.initial_noiseblend = 1.0\n",
    "        #if options.initial_noiseblend < 1.0:\n",
    "        #    initial = content_image\n",
    "\n",
    "    if options.checkpoint_output and \"%s\" not in options.checkpoint_output:\n",
    "        parser.error(\"To save intermediate images, the checkpoint output \"\n",
    "                     \"parameter must contain `%s` (e.g. `foo%s.jpg`)\")\n",
    "\n",
    "    # try saving a dummy image to the output path to make sure that it's writable\n",
    "    try:\n",
    "        imsave(options.output, np.zeros((500, 500, 3)))\n",
    "    except:\n",
    "        raise IOError('%s is not writable or does not have a valid file extension for an image file' % options.output)\n",
    "\n",
    "    for iteration, image in stylize(\n",
    "        network=options.network,\n",
    "        initial=initial,\n",
    "        initial_noiseblend=options.initial_noiseblend,\n",
    "        content=content_image,\n",
    "        styles=style_images,\n",
    "        preserve_colors=options.preserve_colors,\n",
    "        iterations=options.iterations,\n",
    "        content_weight=options.content_weight,\n",
    "        content_weight_blend=options.content_weight_blend,\n",
    "        style_weight=options.style_weight,\n",
    "        color_weight=options.color_weight,\n",
    "        affine_weight=options.affine_weight,\n",
    "        edge_weight=options.edge_weight,\n",
    "        style_layer_weight_exp=options.style_layer_weight_exp,\n",
    "        style_blend_weights=style_blend_weights,\n",
    "        tv_weight=options.tv_weight,\n",
    "        learning_rate=options.learning_rate,\n",
    "        beta1=options.beta1,\n",
    "        beta2=options.beta2,\n",
    "        epsilon=options.epsilon,\n",
    "        pooling=options.pooling,\n",
    "        print_iterations=options.print_iterations,\n",
    "        checkpoint_iterations=options.checkpoint_iterations\n",
    "    ):\n",
    "        output_file = None\n",
    "        combined_rgb = image\n",
    "        if iteration is not None:\n",
    "            if options.checkpoint_output:\n",
    "                output_file = options.checkpoint_output % iteration\n",
    "        else:\n",
    "            output_file = options.output\n",
    "        if output_file:\n",
    "            imsave(output_file, combined_rgb)\n",
    "\n",
    "\n",
    "def imread(path):\n",
    "    img = scipy.misc.imread(path).astype(np.float)\n",
    "    if len(img.shape) == 2:\n",
    "        # grayscale\n",
    "        img = np.dstack((img,img,img))\n",
    "    elif img.shape[2] == 4:\n",
    "        # PNG with alpha channel\n",
    "        img = img[:,:,:3]\n",
    "    return img\n",
    "\n",
    "\n",
    "def imsave(path, img):\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(img).save(path, quality=95)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml13",
   "language": "python",
   "name": "ml13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
